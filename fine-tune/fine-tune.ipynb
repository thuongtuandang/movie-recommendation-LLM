{"cells":[{"cell_type":"markdown","metadata":{},"source":["Fine-tuning LLMs need extensive resource. If you don't have enough GPU (at least 16GB), I would recommend to try it on kaggle. We will fine-tune this on the wiki_movies dataset from huggingface. However, because of the limited resources, I can only perform the fine-tuning process on 1% of the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# !pip install -qqq bitsandbytes\n","# !pip install -qqq torch\n","# !pip install  -qqq -U git+https://github.com/huggingface/transformers\n","# !pip install -qqq -U git+https://github.com/huggingface/peft\n","# !pip install -qqq -U git+https://github.com/huggingface/accelerate\n","# !pip install -qqq datasets\n","# !pip install -qqq loralib\n","# !pip install -qqq einops\n","# !pip install -qqq datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import bitsandbytes as bnb\n","import torch\n","import torch.nn as nn\n","import transformers\n","from huggingface_hub import notebook_login\n","from peft import LoraConfig, PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n","from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","from datasets import load_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Load the model with bits and bytes config\n","model = \"GeneZC/MiniChat-3B\"\n","# model = \"mistralai/Mistral-7B-Instruct-v0.1\"\n","# model = \"openai-community/gpt2-xl\"\n","\n","MODEL_NAME = model\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME,\n","    device_map=\"auto\",\n","    # device_map=\"cpu\",\n","    trust_remote_code=True,\n","    quantization_config=bnb_config\n",")\n","\n","# PEFT wrapper the model for training / fine-tuning\n","model = prepare_model_for_kbit_training(model)\n","\n","# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import re\n","# This is to get the numer of layers of our LLM\n","def get_num_layers(model):\n","    # We first define a set\n","    numbers = set()\n","    for name, _ in model.named_parameters():\n","        # Name is of this form: model.layers.2.post_attention_layernorm.weight\n","        # The number 2 means the index of the post attention layer norm\n","        # We use regular expression to parse the number 2\n","        for number in re.findall(r'\\d+', name):\n","            numbers.add(int(number))\n","    # The number of layers is exactly maximum value of the set numbers\n","    return max(numbers)\n","\n","# This is to get the number of parameters of our LLM\n","def get_num_params(model):\n","    num_params = 0\n","    for _, param in model.named_parameters():\n","        num_params += param.numel()\n","    return num_params\n","\n","def get_last_layer_linears(model):\n","    names = []\n","    \n","    num_layers = get_num_layers(model)\n","    for name, module in model.named_modules():\n","        if str(num_layers) in name and not \"encoder\" in name:\n","            if isinstance(module, torch.nn.Linear):\n","                names.append(name)\n","    return names"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Total number of layers and parameters\n","print(get_num_layers(model))\n","# Total number of named params\n","print(get_num_params(model))\n","print(get_last_layer_linears(model))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["config = LoraConfig(\n","    r=2,\n","    lora_alpha=32,\n","    target_modules=get_last_layer_linears(model),\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","# Redefine the model with Lora config\n","model = get_peft_model(model, config)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["generation_config = model.generation_config\n","# max_new_tokens is limited length of the answer\n","generation_config.max_new_tokens = 100\n","# low temperature (0.1) for more predictable/coherent text\n","# high temperature (0.9) for more creative/unpredictable text\n","generation_config.temperature = 0.1\n","# top_p = 0.7 means the next word must have at least 70% chance to appear\n","generation_config.top_p = 0.1\n","generation_config.do_sample = True\n","generation_config.num_return_sequences = 1\n","generation_config.pad_token_id = tokenizer.eos_token_id\n","generation_config.eos_token_id = tokenizer.eos_token_id"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Defie dataset\n","dataset_name = \"wiki_movies\"\n","dataset = load_dataset(dataset_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Tokenize dataset \n","def tokenize_function(examples):\n","    # Tokenize both the questions and answers in the batch\n","    return tokenizer(examples['question'], examples['answer'], truncation=True, padding='max_length', max_length=1000)\n","\n","# tokenized_datasets = dataset.map(tokenize_function)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from datasets import DatasetDict\n","\n","# Split the datasets to reduce their sizes\n","small_train_dataset = dataset['train'].train_test_split(test_size=0.99)\n","small_validation_dataset = dataset['validation'].train_test_split(test_size=0.9)\n","small_test_dataset = dataset['test'].train_test_split(test_size=0.9)\n","\n","# Note: After splitting, we need to use ['train'] to access the reduced part we want\n","tokenized_train_datasets = small_train_dataset['train'].map(tokenize_function, batched=True)\n","tokenized_validation_datasets = small_validation_dataset['train'].map(tokenize_function, batched=True)\n","tokenized_test_datasets = small_test_dataset['train'].map(tokenize_function, batched=True)\n","\n","# Corrected DatasetDict with matched variable names\n","tokenized_datasets = DatasetDict({\n","    'train': tokenized_train_datasets,\n","    'test': tokenized_test_datasets,\n","    'validation': tokenized_validation_datasets\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","\n","training_args = transformers.TrainingArguments(\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=4,\n","    num_train_epochs=1,\n","    learning_rate=1e-4,\n","    fp16=True,\n","    output_dir=\"finetune_reddit\",\n","    optim=\"paged_adamw_8bit\",\n","    lr_scheduler_type=\"cosine\",\n","    warmup_ratio=0.01,\n","    report_to=\"none\"\n",")\n","\n","trainer = transformers.Trainer(\n","    model=model,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    args=training_args,\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",")\n","model.config.use_cache = False\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.save_pretrained(\"trained-model\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
